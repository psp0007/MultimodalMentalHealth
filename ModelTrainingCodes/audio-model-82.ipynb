{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2d43a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T08:53:31.619550Z",
     "iopub.status.busy": "2025-04-05T08:53:31.619267Z",
     "iopub.status.idle": "2025-04-05T08:59:42.648589Z",
     "shell.execute_reply": "2025-04-05T08:59:42.647698Z"
    },
    "papermill": {
     "duration": 371.033916,
     "end_time": "2025-04-05T08:59:42.649905",
     "exception": false,
     "start_time": "2025-04-05T08:53:31.615989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting audio emotion recognition training...\n",
      "Processing Suprised files...\n",
      "Processing Disgusted files...\n",
      "Processing Angry files...\n",
      "Processing Happy files...\n",
      "Loaded emotion classes: ['Angry' 'Disgusted' 'Happy' 'Suprised']\n",
      "Training set: (5431, 130, 160)\n",
      "Validation set: (679, 130, 160)\n",
      "Test set: (679, 130, 160)\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3921 - loss: 1.6854\n",
      "Epoch 1: val_accuracy improved from -inf to 0.31959, saving model to best_emotion_model.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.3924 - loss: 1.6843 - val_accuracy: 0.3196 - val_loss: 1.3191 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m167/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4760 - loss: 1.2748\n",
      "Epoch 2: val_accuracy improved from 0.31959 to 0.51399, saving model to best_emotion_model.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.4766 - loss: 1.2730 - val_accuracy: 0.5140 - val_loss: 1.0423 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m168/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5838 - loss: 1.0211\n",
      "Epoch 3: val_accuracy did not improve from 0.51399\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.5838 - loss: 1.0207 - val_accuracy: 0.4742 - val_loss: 1.4845 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m168/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6211 - loss: 0.9026\n",
      "Epoch 4: val_accuracy did not improve from 0.51399\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.6214 - loss: 0.9020 - val_accuracy: 0.5007 - val_loss: 1.3778 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m168/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6669 - loss: 0.8168\n",
      "Epoch 5: val_accuracy improved from 0.51399 to 0.58027, saving model to best_emotion_model.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6672 - loss: 0.8161 - val_accuracy: 0.5803 - val_loss: 0.9253 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m167/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6994 - loss: 0.7428\n",
      "Epoch 6: val_accuracy did not improve from 0.58027\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.6994 - loss: 0.7425 - val_accuracy: 0.3314 - val_loss: 2.6127 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m167/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7097 - loss: 0.6871\n",
      "Epoch 7: val_accuracy improved from 0.58027 to 0.64359, saving model to best_emotion_model.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7098 - loss: 0.6870 - val_accuracy: 0.6436 - val_loss: 0.7772 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m167/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7200 - loss: 0.6730\n",
      "Epoch 8: val_accuracy did not improve from 0.64359\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7203 - loss: 0.6723 - val_accuracy: 0.4875 - val_loss: 1.5412 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m168/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7434 - loss: 0.6186\n",
      "Epoch 9: val_accuracy did not improve from 0.64359\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7435 - loss: 0.6182 - val_accuracy: 0.3579 - val_loss: 2.4626 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7431 - loss: 0.6004\n",
      "Epoch 10: val_accuracy did not improve from 0.64359\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7431 - loss: 0.6003 - val_accuracy: 0.3770 - val_loss: 2.6195 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m166/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7593 - loss: 0.5898\n",
      "Epoch 11: val_accuracy did not improve from 0.64359\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7595 - loss: 0.5894 - val_accuracy: 0.3432 - val_loss: 2.0023 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m167/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7761 - loss: 0.5594\n",
      "Epoch 12: val_accuracy did not improve from 0.64359\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7762 - loss: 0.5593 - val_accuracy: 0.6200 - val_loss: 1.1910 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m166/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7909 - loss: 0.5271\n",
      "Epoch 13: val_accuracy improved from 0.64359 to 0.77467, saving model to best_emotion_model.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7911 - loss: 0.5264 - val_accuracy: 0.7747 - val_loss: 0.5623 - learning_rate: 2.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m165/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8100 - loss: 0.4703\n",
      "Epoch 14: val_accuracy did not improve from 0.77467\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8100 - loss: 0.4703 - val_accuracy: 0.7393 - val_loss: 0.5932 - learning_rate: 2.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m166/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8101 - loss: 0.4674\n",
      "Epoch 15: val_accuracy improved from 0.77467 to 0.82032, saving model to best_emotion_model.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8103 - loss: 0.4671 - val_accuracy: 0.8203 - val_loss: 0.5016 - learning_rate: 2.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8241 - loss: 0.4447\n",
      "Epoch 16: val_accuracy did not improve from 0.82032\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8241 - loss: 0.4447 - val_accuracy: 0.8041 - val_loss: 0.5411 - learning_rate: 2.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m167/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8271 - loss: 0.4388\n",
      "Epoch 17: val_accuracy did not improve from 0.82032\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8271 - loss: 0.4386 - val_accuracy: 0.7555 - val_loss: 0.5900 - learning_rate: 2.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m168/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8278 - loss: 0.4302\n",
      "Epoch 18: val_accuracy did not improve from 0.82032\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8278 - loss: 0.4302 - val_accuracy: 0.7010 - val_loss: 0.7257 - learning_rate: 2.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m169/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8406 - loss: 0.4147\n",
      "Epoch 19: val_accuracy did not improve from 0.82032\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8407 - loss: 0.4147 - val_accuracy: 0.7364 - val_loss: 0.6792 - learning_rate: 2.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m166/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8477 - loss: 0.4076\n",
      "Epoch 20: val_accuracy did not improve from 0.82032\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8475 - loss: 0.4076 - val_accuracy: 0.7526 - val_loss: 0.5973 - learning_rate: 2.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8409 - loss: 0.4143\n",
      "Epoch 21: val_accuracy improved from 0.82032 to 0.83211, saving model to best_emotion_model.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8410 - loss: 0.4142 - val_accuracy: 0.8321 - val_loss: 0.4386 - learning_rate: 4.0000e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m167/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8508 - loss: 0.3889\n",
      "Epoch 22: val_accuracy improved from 0.83211 to 0.83358, saving model to best_emotion_model.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8507 - loss: 0.3889 - val_accuracy: 0.8336 - val_loss: 0.4422 - learning_rate: 4.0000e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m166/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8539 - loss: 0.3860\n",
      "Epoch 23: val_accuracy did not improve from 0.83358\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8538 - loss: 0.3859 - val_accuracy: 0.8233 - val_loss: 0.4766 - learning_rate: 4.0000e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m166/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8510 - loss: 0.3841\n",
      "Epoch 24: val_accuracy did not improve from 0.83358\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8509 - loss: 0.3841 - val_accuracy: 0.8336 - val_loss: 0.4754 - learning_rate: 4.0000e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m166/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8492 - loss: 0.3841\n",
      "Epoch 25: val_accuracy improved from 0.83358 to 0.84242, saving model to best_emotion_model.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8493 - loss: 0.3839 - val_accuracy: 0.8424 - val_loss: 0.4431 - learning_rate: 4.0000e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m166/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8494 - loss: 0.3782\n",
      "Epoch 26: val_accuracy did not improve from 0.84242\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8495 - loss: 0.3780 - val_accuracy: 0.8262 - val_loss: 0.4690 - learning_rate: 4.0000e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m165/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8556 - loss: 0.3754\n",
      "Epoch 27: val_accuracy did not improve from 0.84242\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8556 - loss: 0.3750 - val_accuracy: 0.8144 - val_loss: 0.4782 - learning_rate: 1.0000e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m168/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8645 - loss: 0.3716\n",
      "Epoch 28: val_accuracy did not improve from 0.84242\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8645 - loss: 0.3715 - val_accuracy: 0.8218 - val_loss: 0.4693 - learning_rate: 1.0000e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m165/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8590 - loss: 0.3700\n",
      "Epoch 29: val_accuracy did not improve from 0.84242\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8591 - loss: 0.3695 - val_accuracy: 0.8174 - val_loss: 0.4634 - learning_rate: 1.0000e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m166/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8625 - loss: 0.3584\n",
      "Epoch 30: val_accuracy did not improve from 0.84242\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8623 - loss: 0.3585 - val_accuracy: 0.8292 - val_loss: 0.4554 - learning_rate: 1.0000e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m168/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8658 - loss: 0.3719\n",
      "Epoch 31: val_accuracy did not improve from 0.84242\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8658 - loss: 0.3717 - val_accuracy: 0.8321 - val_loss: 0.4518 - learning_rate: 1.0000e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m167/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8611 - loss: 0.3650\n",
      "Epoch 32: val_accuracy did not improve from 0.84242\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8611 - loss: 0.3648 - val_accuracy: 0.8247 - val_loss: 0.4557 - learning_rate: 1.0000e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m168/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8691 - loss: 0.3577\n",
      "Epoch 33: val_accuracy did not improve from 0.84242\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8690 - loss: 0.3576 - val_accuracy: 0.8277 - val_loss: 0.4573 - learning_rate: 1.0000e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m166/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8619 - loss: 0.3709\n",
      "Epoch 34: val_accuracy did not improve from 0.84242\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8619 - loss: 0.3708 - val_accuracy: 0.8409 - val_loss: 0.4315 - learning_rate: 1.0000e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m167/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8643 - loss: 0.3655\n",
      "Epoch 35: val_accuracy did not improve from 0.84242\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8642 - loss: 0.3655 - val_accuracy: 0.8395 - val_loss: 0.4392 - learning_rate: 1.0000e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m166/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8557 - loss: 0.3589\n",
      "Epoch 36: val_accuracy did not improve from 0.84242\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8557 - loss: 0.3590 - val_accuracy: 0.8233 - val_loss: 0.4565 - learning_rate: 1.0000e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m168/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8694 - loss: 0.3620\n",
      "Epoch 37: val_accuracy did not improve from 0.84242\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8694 - loss: 0.3619 - val_accuracy: 0.8365 - val_loss: 0.4433 - learning_rate: 1.0000e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8637 - loss: 0.3681\n",
      "Epoch 38: val_accuracy did not improve from 0.84242\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8637 - loss: 0.3681 - val_accuracy: 0.8321 - val_loss: 0.4507 - learning_rate: 1.0000e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m165/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8601 - loss: 0.3563\n",
      "Epoch 39: val_accuracy did not improve from 0.84242\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8602 - loss: 0.3562 - val_accuracy: 0.8395 - val_loss: 0.4504 - learning_rate: 1.0000e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m167/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8697 - loss: 0.3569\n",
      "Epoch 40: val_accuracy did not improve from 0.84242\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8696 - loss: 0.3567 - val_accuracy: 0.8409 - val_loss: 0.4400 - learning_rate: 1.0000e-05\n",
      "Epoch 40: early stopping\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Evaluating model...\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8419 - loss: 0.4131\n",
      "Test accuracy: 0.8292\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.90      0.80      0.84       217\n",
      "   Disgusted       0.75      0.87      0.81       186\n",
      "       Happy       0.82      0.80      0.81       217\n",
      "    Suprised       0.92      0.92      0.92        59\n",
      "\n",
      "    accuracy                           0.83       679\n",
      "   macro avg       0.85      0.85      0.84       679\n",
      "weighted avg       0.83      0.83      0.83       679\n",
      "\n",
      "Saving model...\n",
      "Model and label encoder saved successfully.\n",
      "Training complete!\n",
      "Final test accuracy: 0.8292\n",
      "\n",
      "Example prediction:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "Predicted emotion: Happy with confidence: 0.8111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Constants\n",
    "DATASET_PATH = '/kaggle/input/audio-emotions/Emotions'\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 3\n",
    "MAX_SAMPLES = SAMPLE_RATE * DURATION\n",
    "TARGET_EMOTIONS = ['Angry', 'Happy', 'Suprised', 'Disgusted']\n",
    "\n",
    "# Feature extraction\n",
    "def extract_features(file_path):\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "        if len(audio) < MAX_SAMPLES:\n",
    "            audio = np.pad(audio, (0, MAX_SAMPLES - len(audio)), 'constant')\n",
    "        else:\n",
    "            audio = audio[:MAX_SAMPLES]\n",
    "\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=SAMPLE_RATE, n_mfcc=13)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=SAMPLE_RATE)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=SAMPLE_RATE)\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio, sr=SAMPLE_RATE)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        features = np.concatenate([mfccs, spectral_contrast, chroma, mel_spec_db], axis=0).T\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features from {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Prepare dataset with filtering\n",
    "def prepare_dataset():\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for emotion_folder in os.listdir(DATASET_PATH):\n",
    "        if emotion_folder not in TARGET_EMOTIONS:\n",
    "            continue\n",
    "        \n",
    "        emotion_path = os.path.join(DATASET_PATH, emotion_folder)\n",
    "        if not os.path.isdir(emotion_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {emotion_folder} files...\")\n",
    "        for audio_file in os.listdir(emotion_path):\n",
    "            if audio_file.endswith('.wav'):\n",
    "                file_path = os.path.join(emotion_path, audio_file)\n",
    "                audio_features = extract_features(file_path)\n",
    "                if audio_features is not None:\n",
    "                    features.append(audio_features)\n",
    "                    labels.append(emotion_folder)\n",
    "\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    print(\"Loaded emotion classes:\", np.unique(labels))\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "    categorical_labels = to_categorical(encoded_labels)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, categorical_labels, test_size=0.2, random_state=42, stratify=categorical_labels\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, label_encoder\n",
    "\n",
    "# Build model\n",
    "def build_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=3, activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Conv1D(128, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Conv1D(256, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        LSTM(128, return_sequences=False),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train model\n",
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    num_classes = y_train.shape[1]\n",
    "    model = build_model(input_shape, num_classes)\n",
    "\n",
    "    checkpoint = ModelCheckpoint('best_emotion_model.keras', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[checkpoint, early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, history, X_test, y_test, label_encoder):\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    class_names = label_encoder.classes_\n",
    "    report = classification_report(y_true_classes, y_pred_classes, target_names=class_names)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "\n",
    "    return accuracy, report\n",
    "\n",
    "# Save model and encoder\n",
    "def save_model(model, label_encoder):\n",
    "    model.save('emotion_recognition_model.keras')\n",
    "    np.save('label_encoder_classes.npy', label_encoder.classes_)\n",
    "    print(\"Model and label encoder saved successfully.\")\n",
    "\n",
    "# Predict emotion for new audio\n",
    "def predict_emotion(file_path, model, label_encoder):\n",
    "    features = extract_features(file_path)\n",
    "    features = np.expand_dims(features, axis=0)\n",
    "    prediction = model.predict(features)[0]\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    predicted_emotion = label_encoder.classes_[predicted_class]\n",
    "    confidence = prediction[predicted_class]\n",
    "    return predicted_emotion, confidence\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting audio emotion recognition training...\")\n",
    "\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = prepare_dataset()\n",
    "    print(f\"Training set: {X_train.shape}\")\n",
    "    print(f\"Validation set: {X_val.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    model, history = train_model(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, report = evaluate_model(model, history, X_test, y_test, label_encoder)\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    save_model(model, label_encoder)\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Final test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Example usage\n",
    "    print(\"\\nExample prediction:\")\n",
    "    example_file = os.path.join(DATASET_PATH, 'Happy', os.listdir(os.path.join(DATASET_PATH, 'Happy'))[0])\n",
    "    emotion, confidence = predict_emotion(example_file, model, label_encoder)\n",
    "    print(f\"Predicted emotion: {emotion} with confidence: {confidence:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da23950",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T08:59:42.777488Z",
     "iopub.status.busy": "2025-04-05T08:59:42.776879Z",
     "iopub.status.idle": "2025-04-05T08:59:43.138665Z",
     "shell.execute_reply": "2025-04-05T08:59:43.137460Z"
    },
    "papermill": {
     "duration": 0.425443,
     "end_time": "2025-04-05T08:59:43.140005",
     "exception": true,
     "start_time": "2025-04-05T08:59:42.714562",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤ Emotion Recognition Testing\n",
      "Give path to a .wav file or type 'exit' to stop.\n",
      "\n"
     ]
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e7d11cbf037a>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-e7d11cbf037a>\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸ” Enter .wav file path: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \"\"\"\n\u001b[1;32m    847\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_stdin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m             raise StdinNotImplementedError(\n\u001b[0m\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load trained model and label encoder classes\n",
    "model = load_model('/kaggle/working/emotion_recognition_model.keras')\n",
    "label_classes = np.load('/kaggle/working/label_encoder_classes.npy')\n",
    "\n",
    "# Constants\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 3\n",
    "MAX_SAMPLES = SAMPLE_RATE * DURATION\n",
    "\n",
    "# Feature extraction (same as training)\n",
    "def extract_features(file_path):\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "        if len(audio) < MAX_SAMPLES:\n",
    "            audio = np.pad(audio, (0, MAX_SAMPLES - len(audio)), 'constant')\n",
    "        else:\n",
    "            audio = audio[:MAX_SAMPLES]\n",
    "\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=SAMPLE_RATE, n_mfcc=13)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=SAMPLE_RATE)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=SAMPLE_RATE)\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio, sr=SAMPLE_RATE)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        features = np.concatenate([mfccs, spectral_contrast, chroma, mel_spec_db], axis=0)\n",
    "        features = features.T\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Feature extraction failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Prediction\n",
    "def predict_emotion(file_path):\n",
    "    features = extract_features(file_path)\n",
    "    if features is None:\n",
    "        return None, None\n",
    "    features = np.expand_dims(features, axis=0)\n",
    "    prediction = model.predict(features)[0]\n",
    "    predicted_index = np.argmax(prediction)\n",
    "    predicted_emotion = label_classes[predicted_index]\n",
    "    confidence = prediction[predicted_index]\n",
    "    return predicted_emotion, confidence\n",
    "\n",
    "# Continuous testing loop\n",
    "def test_loop():\n",
    "    print(\"ğŸ¤ Emotion Recognition Testing\")\n",
    "    print(\"Give path to a .wav file or type 'exit' to stop.\\n\")\n",
    "\n",
    "    while True:\n",
    "        file_path = input(\"ğŸ” Enter .wav file path: \").strip()\n",
    "\n",
    "        if file_path.lower() == 'exit':\n",
    "            print(\"ğŸ‘‹ Exiting testing loop.\")\n",
    "            break\n",
    "\n",
    "        if not os.path.isfile(file_path) or not file_path.endswith('.wav'):\n",
    "            print(\"âš ï¸  Invalid file path. Please provide a valid .wav file.\\n\")\n",
    "            continue\n",
    "\n",
    "        emotion, confidence = predict_emotion(file_path)\n",
    "        if emotion:\n",
    "            print(f\"âœ… Emotion: {emotion} | ğŸ”¥ Confidence: {confidence:.2f}\\n\")\n",
    "        else:\n",
    "            print(\"âŒ Could not predict emotion.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_loop()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 703366,
     "sourceId": 1229041,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 861334,
     "sourceId": 1468503,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3509705,
     "sourceId": 6232045,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 376.897783,
   "end_time": "2025-04-05T08:59:45.993352",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-05T08:53:29.095569",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
